{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alibi Detect Version: 0.10.4\n",
      "Torch Version: 1.13.0+cu117\n",
      "Torchvision Version: 0.14.0+cu117\n",
      "Wilds Version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# alibi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from alibi_detect.cd import KSDrift\n",
    "from alibi_detect.models.tensorflow import scale_by_instance\n",
    "from alibi_detect.utils.fetching import fetch_tf_model, fetch_detector\n",
    "from alibi_detect.saving import save_detector, load_detector\n",
    "\n",
    "# wilds\n",
    "from typing import Tuple, Generator, Callable, Optional\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds import get_dataset\n",
    "import alibi_detect\n",
    "import torchvision\n",
    "import wilds\n",
    "from alibi_detect.models.pytorch import trainer\n",
    "\n",
    "print(f\"Alibi Detect Version: {alibi_detect.__version__}\")\n",
    "print(f\"Torch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "print(f\"Wilds Version: {wilds.__version__}\")\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cpu')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jinglewsl/evoila/projects/image-drift-monitoring\n",
      "here\n",
      "(5000, 3, 96, 96)\n",
      "(5000, 3, 96, 96)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# datensatz camelyon laden\n",
    "%cd  '/home/jinglewsl/evoila/projects/image-drift-monitoring'\n",
    "%pwd\n",
    "camelyon = get_dataset(dataset=\"camelyon17\",download=False)\n",
    "\n",
    "WILDS_PATH = 'data'\n",
    "N = 5000 # Size of reference set\n",
    "DOWNLOAD = False\n",
    "img_size = (96,96)\n",
    "ds_train = camelyon.get_subset('train',transform=transforms.Compose([transforms.Resize(img_size), transforms.ToTensor()]))\n",
    "train_loader = iter(get_train_loader('standard',ds_train,1))\n",
    "\n",
    "ds_test = camelyon.get_subset('test',transform=transforms.Compose([transforms.Resize(img_size), transforms.ToTensor()]))\n",
    "# test_loader = iter(get_train_loader('standard',ds_test,1))\n",
    "\n",
    "x_ref = np.stack([next(train_loader)[0][0].numpy() for _ in range(N)],axis=0)\n",
    "# x_test = np.stack([next(test_loader)[0][0].numpy() for _ in range(N)],axis=0)\n",
    "\n",
    "train_loader_train = get_train_loader('standard',ds_train,1)\n",
    "\n",
    "# x_ref = np.stack(data,axis=0)\n",
    "print('here')\n",
    "tmp = np.stack([tensor[0][0].numpy() for tensor ,i in zip(train_loader_train,range(N)) if i <N],axis=0)\n",
    "# for labeled_batch in train_loader_train:\n",
    "#     test = labeled_batch[0][0].numpy() \n",
    "#     tmp = np.stack(test,axis=0)    \n",
    "\n",
    "#     i +=1\n",
    "#     if i >=1:\n",
    "        # break\n",
    "print(x_ref.shape)\n",
    "print(tmp.shape)\n",
    "print(type(x_ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 3, 96, 96) (2500, 3, 96, 96)\n"
     ]
    }
   ],
   "source": [
    "# We split the original test set in a reference dataset and a dataset which should not be rejected under the H0 of the K-S test.\n",
    "# We also split the corrupted data by corruption type:\n",
    "np.random.seed(0)\n",
    "n_test = x_ref.shape[0]\n",
    "idx = np.random.choice(n_test, size=n_test // 2, replace=False)\n",
    "idx_h0 = np.delete(np.arange(n_test), idx, axis=0)\n",
    "X_ref = x_ref[idx]\n",
    "X_h0= x_ref[idx_h0]\n",
    "print(X_ref.shape, X_h0.shape)\n",
    "# print(x_ref.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Random encoder (Untrained Auto Encoder)\n",
    "from functools import partial\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, InputLayer, Reshape\n",
    "from alibi_detect.cd.tensorflow import preprocess_drift\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# define encoder\n",
    "encoding_dim = 96\n",
    "encoder_net = tf.keras.Sequential(\n",
    "  [\n",
    "      InputLayer(input_shape=(3,96,96)),\n",
    "      Conv2D(128, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2D(256, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Conv2D(512, 4, strides=2, padding='same', activation=tf.nn.relu),\n",
    "      Flatten(),\n",
    "      Dense(encoding_dim,)\n",
    "  ]\n",
    ")\n",
    "\n",
    "# define preprocessing function\n",
    "preprocess_fn = partial(preprocess_drift, model=encoder_net, batch_size=512)\n",
    "\n",
    "\n",
    "# initialise drift detector\n",
    "p_val = .05\n",
    "cd = KSDrift(X_ref, p_val=p_val, preprocess_fn=preprocess_fn)\n",
    "\n",
    "# we can also save/load an initialised detector\n",
    "filepath = '/home/jinglewsl/evoila/projects/image-drift-monitoring/src/modules/alibi_detect/pca_ks_detector_camelyon'  # change to directory where detector is saved\n",
    "save_detector(cd, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cd.p_val / cd.n_features == p_val / encoding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Letâ€™s check whether the detector thinks drift occurred on the different test sets and time the prediction calls:\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "labels = ['No!', 'Yes!']\n",
    "\n",
    "def make_predictions(cd, x_h0,x_corr=None):\n",
    "    t = timer()\n",
    "    preds = cd.predict(x_h0)\n",
    "    dt = timer() - t\n",
    "    print('No corruption')\n",
    "    print('Drift? {}'.format(labels[preds['data']['is_drift']]))\n",
    "    print('Feature-wise p-values:')\n",
    "    print(preds['data']['p_val'])\n",
    "    print('len:{}'.format(len(preds['data']['p_val'])))\n",
    "    print(f'Time (s) {dt:.3f}')\n",
    "\n",
    "\n",
    "    if len(x_corr) > 0:\n",
    "        t = timer()\n",
    "        preds_2 = cd.predict(x_corr)\n",
    "        dt = timer() - t\n",
    "        print('')\n",
    "        print('Drift? {}'.format(labels[preds_2['data']['is_drift']]))\n",
    "        print('Feature-wise p-values:')\n",
    "        print(preds_2['data']['p_val'])\n",
    "        print('len:{}'.format(len(preds_2['data']['p_val'])))\n",
    "        print(f'Time (s) {dt:.3f}')\n",
    "        return preds_2  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# We split the original test set in a reference dataset and a dataset which should not be rejected under the H0 of the K-S test.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# We also split the corrupted data by corruption type:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(\u001b[39m0\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m n_test \u001b[39m=\u001b[39m x_test\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(n_test, size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m idx_h0 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdelete(np\u001b[39m.\u001b[39marange(n_test), idx, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "# We split the original test set in a reference dataset and a dataset which should not be rejected under the H0 of the K-S test.\n",
    "# We also split the corrupted data by corruption type:\n",
    "np.random.seed(0)\n",
    "n_test = x_test.shape[0]\n",
    "idx = np.random.choice(n_test, size=2, replace=False)\n",
    "idx_h0 = np.delete(np.arange(n_test), idx, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "x_test_subset = x_test[idx] \n",
    "\n",
    "print(len(x_test))\n",
    "print(len(x_test_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_predictions(cd, X_ref,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_2 =make_predictions(cd,x_ref,x_test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_above_threshold = []\n",
    "p_below_threshold = []\n",
    "for p_val_pred in preds_2['data']['p_val']:\n",
    "    if p_val_pred < p_val:\n",
    "        p_below_threshold.append(p_val_pred)\n",
    "    else:\n",
    "        p_above_threshold.append(p_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p_above_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p_below_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(nrows=1,ncols=6,figsize=(15,4))\n",
    "for i in range(6):\n",
    "    axis[i].imshow(np.transpose(x_ref[i],(1,2,0)))\n",
    "    axis[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(nrows=1,ncols=6,figsize=(15,4))\n",
    "for i in range(6):\n",
    "    axis[i].imshow(np.transpose(x_test[i],(1,2,0)))\n",
    "    axis[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "in_array = [0, math.pi / 2, np.pi / 3, np.pi]\n",
    "out_array = np.tanh(in_array)\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "z = 1/(1 + np.exp(-x))\n",
    "plt.plot(x,z)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"Sigmoid(X)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x))\n",
    "    ds=s*(1-s)  \n",
    "    return s,ds\n",
    "x=np.arange(-6,6,0.01)\n",
    "sigmoid(x)\n",
    "# Setup centered axes\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.spines['left'].set_position('center')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "# Create and show plot\n",
    "ax.plot(x,sigmoid(x)[0], color=\"#307EC7\", linewidth=3, label=\"sigmoid\")\n",
    "ax.plot(x,sigmoid(x)[1], color=\"#9621E2\", linewidth=3, label=\"derivative\")\n",
    "ax.legend(loc=\"upper right\", frameon=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    s=1/(1+np.exp(-x))\n",
    "    ds=s*(1-s)  \n",
    "    return s,ds\n",
    "x=np.arange(-4,4,0.01)\n",
    "sigmoid(x)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    " return np.maximum(0, x)\n",
    "def tanh(x):\n",
    "    t=(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    dt=1-t**2\n",
    "    return t,dt\n",
    "z=np.arange(-4,4,0.01)\n",
    "tanh(z)[0].size,tanh(z)[1].size\n",
    "# Setup centered axes\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "# ax.spines['left'].set_position('center')\n",
    "# ax.spines['bottom'].set_position('center')\n",
    "# ax.spines['right'].set_color('none')\n",
    "# ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "# Create and show plot\n",
    "ax.plot(z,tanh(z)[0], color=\"#307EC7\", linewidth=3, label=\"tanh\")\n",
    "ax.plot(x,sigmoid(x)[0], color=\"#9621E2\", linewidth=3, label=\"sigmoid\")\n",
    "# ax.plot(z,tanh(z)[1], color=\"#9621E2\", linewidth=3, label=\"derivative\")\n",
    "ax.plot(z,relu(z),color=\"#000000\",linewidth=3, label=\"ReLU\")\n",
    "ax.legend(loc=\"upper left\", frameon=False)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whytorchENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d5e5b4e2a2011e1eceda9b955a8ee698f9d79ba930b51ac262fe20fc0d97bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
